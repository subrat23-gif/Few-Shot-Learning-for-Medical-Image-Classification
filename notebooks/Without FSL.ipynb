{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3hO9N0H4w_NZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUy2VdRJ3Cyk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Stratified Split ---\n",
            "Found 4 classes: ['COVID', 'Normal', 'Pneumonia', 'Tuberculosis']\n",
            "\n",
            "Total images found: 104329\n",
            "Original Class distribution: Counter({'Normal': 91225, 'Pneumonia': 8788, 'COVID': 3616, 'Tuberculosis': 700})\n",
            "\n",
            "Splitting data into:\n",
            "  Train set: 83463 images\n",
            "  Val set:   10433 images\n",
            "  Test set:  10433 images\n",
            "\n",
            "Successfully created stratified split at: data_split/\n",
            "--- Data Splitting Complete ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "\n",
        "# This is your *original* folder with 91k+ images\n",
        "SOURCE_DATA_DIR = r'C:\\Users\\SUBRAT\\MAFSL PROJECT\\processed_data\\processed_data'\n",
        "\n",
        "# This is the *new* folder where the 80/10/10 split will be created\n",
        "TARGET_DATA_DIR = 'data_split/'\n",
        "\n",
        "# Define the split ratios\n",
        "VAL_SIZE = 0.1  # 10% for validation\n",
        "TEST_SIZE = 0.1 # 10% for testing\n",
        "# (The remaining 80% will be for training)\n",
        "\n",
        "RANDOM_STATE = 42 # Ensures the split is repeatable\n",
        "\n",
        "# --- 2. The Splitting Function ---\n",
        "\n",
        "def create_stratified_split(source_dir, target_dir):\n",
        "    \"\"\"\n",
        "    Finds all images in the source_dir, splits them into 80/10/10\n",
        "    train/val/test sets with stratification, and copies them to the\n",
        "    new target_dir.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"--- Starting Stratified Split ---\")\n",
        "    \n",
        "    if os.path.exists(target_dir):\n",
        "        print(f\"Removing existing directory: {target_dir}\")\n",
        "        shutil.rmtree(target_dir)\n",
        "\n",
        "    # 1. Find all image paths and their corresponding labels\n",
        "    all_filepaths = []\n",
        "    all_labels = []\n",
        "    \n",
        "    # We must use sorted() to ensure class order is consistent\n",
        "    # (e.g., 'COVID' is always 0, 'NORMAL' is 1, etc.)\n",
        "    classes = sorted([d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))])\n",
        "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "    \n",
        "    if not classes:\n",
        "        print(f\"Error: No subdirectories found in {source_dir}. Aborting.\")\n",
        "        return False\n",
        "        \n",
        "    print(f\"Found {len(classes)} classes: {classes}\")\n",
        "\n",
        "    for cls in classes:\n",
        "        class_dir = os.path.join(source_dir, cls)\n",
        "        # Check if it's a directory\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "            \n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            \n",
        "            # Check if it's a file\n",
        "            if os.path.isfile(img_path):\n",
        "                all_filepaths.append(img_path)\n",
        "                all_labels.append(class_to_idx[cls])\n",
        "\n",
        "    print(f\"\\nTotal images found: {len(all_filepaths)}\")\n",
        "    if not all_filepaths:\n",
        "        print(\"Error: No image files found. Check your 'processed_data' folder.\")\n",
        "        return False\n",
        "        \n",
        "    print(f\"Original Class distribution: {Counter([classes[i] for i in all_labels])}\")\n",
        "\n",
        "    # 2. Create the first split (train_val vs. test)\n",
        "    # We stratify on the labels to keep class ratios the same\n",
        "    train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
        "        all_filepaths, all_labels, \n",
        "        test_size=TEST_SIZE, \n",
        "        stratify=all_labels, \n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # 3. Create the second split (train vs. val)\n",
        "    # We adjust the validation size relative to the 90% that's left\n",
        "    val_size_adjusted = VAL_SIZE / (1.0 - TEST_SIZE)\n",
        "    \n",
        "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "        train_val_files, train_val_labels, \n",
        "        test_size=val_size_adjusted, \n",
        "        stratify=train_val_labels, \n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nSplitting data into:\")\n",
        "    print(f\"  Train set: {len(train_files)} images\")\n",
        "    print(f\"  Val set:   {len(val_files)} images\")\n",
        "    print(f\"  Test set:  {len(test_files)} images\")\n",
        "\n",
        "    # 4. Copy files to new directories (train/, val/, test/)\n",
        "    datasets = {\n",
        "        'train': (train_files, train_labels),\n",
        "        'val': (val_files, val_labels),\n",
        "        'test': (test_files, test_labels)\n",
        "    }\n",
        "\n",
        "    for split_name, (files, labels) in datasets.items():\n",
        "        split_path = os.path.join(target_dir, split_name)\n",
        "        \n",
        "        for i, filepath in enumerate(files):\n",
        "            # Get the class name\n",
        "            class_name = classes[labels[i]]\n",
        "            \n",
        "            # Create target class dir (e.g., data_split/train/COVID/)\n",
        "            target_class_dir = os.path.join(split_path, class_name)\n",
        "            os.makedirs(target_class_dir, exist_ok=True)\n",
        "            \n",
        "            # Copy file\n",
        "            shutil.copy(filepath, target_class_dir)\n",
        "            \n",
        "    print(f\"\\nSuccessfully created stratified split at: {target_dir}\")\n",
        "    print(\"--- Data Splitting Complete ---\")\n",
        "    return True\n",
        "\n",
        "# --- 3. Run the Function ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if scikit-learn is installed\n",
        "    try:\n",
        "        from sklearn.model_selection import train_test_split\n",
        "    except ImportError:\n",
        "        print(\"Error: scikit-learn is not installed.\")\n",
        "        print(\"Please install it by running: pip install scikit-learn\")\n",
        "        exit()\n",
        "\n",
        "    if not os.path.exists(SOURCE_DATA_DIR):\n",
        "        print(f\"Error: Source directory not found: {SOURCE_DATA_DIR}\")\n",
        "        print(\"Please make sure your data is in a folder named 'processed_data' in the same directory.\")\n",
        "    else:\n",
        "        create_stratified_sp\n",
        "        lit(SOURCE_DATA_DIR, TARGET_DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Imbalanced-Aware Training ---\n",
            "\n",
            "Training on 4 classes: ['COVID', 'Normal', 'Pneumonia', 'Tuberculosis']\n",
            "\n",
            "Calculating sampler weights for training set...\n",
            "  Class counts: [2893, 72980, 7030, 560]\n",
            "  Class weights: tensor([3.4566e-04, 1.3702e-05, 1.4225e-04, 1.7857e-03])\n",
            "WeightedRandomSampler created.\n",
            "\n",
            "Starting training for 10 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 2609/2609 [1:42:28<00:00,  2.36s/it]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 327/327 [10:14<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "  Train Loss: 0.5707\n",
            "  Val Loss:   0.4799\n",
            "  Val F1 (Weighted): 0.8150\n",
            "  New best model saved to best_imbalanced_model.pth (F1: 0.8150)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 2609/2609 [1:38:01<00:00,  2.25s/it]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 327/327 [10:19<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "  Train Loss: 0.3428\n",
            "  Val Loss:   0.4237\n",
            "  Val F1 (Weighted): 0.8389\n",
            "  New best model saved to best_imbalanced_model.pth (F1: 0.8389)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 2609/2609 [1:37:21<00:00,  2.24s/it]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 327/327 [10:07<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "  Train Loss: 0.3025\n",
            "  Val Loss:   0.4596\n",
            "  Val F1 (Weighted): 0.8279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 2609/2609 [1:36:40<00:00,  2.22s/it]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 327/327 [10:10<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "  Train Loss: 0.2833\n",
            "  Val Loss:   0.3894\n",
            "  Val F1 (Weighted): 0.8497\n",
            "  New best model saved to best_imbalanced_model.pth (F1: 0.8497)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 2609/2609 [1:36:42<00:00,  2.22s/it]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 327/327 [10:00<00:00,  1.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "  Train Loss: 0.2699\n",
            "  Val Loss:   0.3309\n",
            "  Val F1 (Weighted): 0.8721\n",
            "  New best model saved to best_imbalanced_model.pth (F1: 0.8721)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 2609/2609 [1:37:06<00:00,  2.23s/it]\n",
            "Epoch 6/10 [Val]: 100%|██████████| 327/327 [11:34<00:00,  2.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "  Train Loss: 0.2601\n",
            "  Val Loss:   0.3747\n",
            "  Val F1 (Weighted): 0.8561\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 2609/2609 [1:41:58<00:00,  2.34s/it]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 327/327 [10:19<00:00,  1.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "  Train Loss: 0.2475\n",
            "  Val Loss:   0.4198\n",
            "  Val F1 (Weighted): 0.8436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 2609/2609 [1:57:26<00:00,  2.70s/it]  \n",
            "Epoch 8/10 [Val]: 100%|██████████| 327/327 [10:18<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "  Train Loss: 0.2434\n",
            "  Val Loss:   0.3403\n",
            "  Val F1 (Weighted): 0.8690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 2609/2609 [1:59:04<00:00,  2.74s/it]    \n",
            "Epoch 9/10 [Val]: 100%|██████████| 327/327 [11:37<00:00,  2.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "  Train Loss: 0.2383\n",
            "  Val Loss:   0.4271\n",
            "  Val F1 (Weighted): 0.8402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 2609/2609 [2:43:56<00:00,  3.77s/it]  \n",
            "Epoch 10/10 [Val]: 100%|██████████| 327/327 [16:19<00:00,  3.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "  Train Loss: 0.2333\n",
            "  Val Loss:   0.3539\n",
            "  Val F1 (Weighted): 0.8652\n",
            "\n",
            "Training complete.\n",
            "\n",
            "--- FINAL EVALUATION ON TEST SET ---\n",
            "Loaded best model from best_imbalanced_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 327/327 [16:22<00:00,  3.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       COVID       0.84      0.91      0.88       362\n",
            "      Normal       0.97      0.85      0.91      9122\n",
            "   Pneumonia       0.35      0.78      0.48       879\n",
            "Tuberculosis       0.88      0.96      0.92        70\n",
            "\n",
            "    accuracy                           0.85     10433\n",
            "   macro avg       0.76      0.88      0.80     10433\n",
            "weighted avg       0.92      0.85      0.87     10433\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 331   21    7    3]\n",
            " [  56 7797 1265    4]\n",
            " [   4  190  683    2]\n",
            " [   2    0    1   67]]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim/\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchxrayvision as xrv\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import warnings\n",
        "\n",
        "# --- 0. Configuration ---\n",
        "# This is the folder you just created with split_data.py\n",
        "SPLIT_DATA_DIR = 'data_split/' \n",
        "\n",
        "# Training Hyperparameters\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "MODEL_SAVE_PATH = 'best_imbalanced_model.pth'\n",
        "\n",
        "# ==============================================================================\n",
        "# STAGE 2: IMBALANCED-AWARE TRAINING\n",
        "# ==============================================================================\n",
        "\n",
        "def get_xrv_transforms():\n",
        "    \"\"\"\n",
        "    Get the specific transforms for torchxrayvision models (1-channel).\n",
        "    \"\"\"\n",
        "    # This is the normalization specified by torchxrayvision\n",
        "    XRV_MEAN = [0.5081]\n",
        "    XRV_STD = [0.0893]\n",
        "    \n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Grayscale(num_output_channels=1), # XRV models expect 1 channel\n",
        "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=XRV_MEAN, std=XRV_STD)\n",
        "    ])\n",
        "    \n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=XRV_MEAN, std=XRV_STD)\n",
        "    ])\n",
        "    \n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "class XRVTransferModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A wrapper for the torchxrayvision model to add a custom classifier head.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super(XRVTransferModel, self).__init__()\n",
        "        # Load pre-trained backbone\n",
        "        # This model is pre-trained on chest x-rays, which is perfect\n",
        "        model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
        "        self.backbone = model.features\n",
        "        \n",
        "        # --- Freeze the backbone ---\n",
        "        # We will only train the final classifier layer\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "              if 'denseblock4' not in name:\n",
        "                      param.requires_grad = False\n",
        "\n",
        "             \n",
        "        # Add a new classifier head\n",
        "        # DenseNet-121 output is 1024 features\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5), # Regularization\n",
        "            nn.Linear(1024, num_classes) # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        pooled = self.pooling(features).view(features.size(0), -1)\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n",
        "\n",
        "def create_weighted_sampler(dataset):\n",
        "    \"\"\"\n",
        "    Creates a WeightedRandomSampler to handle class imbalance.\n",
        "    \"\"\"\n",
        "    print(\"\\nCalculating sampler weights for training set...\")\n",
        "    class_counts = Counter(dataset.targets)\n",
        "    \n",
        "    # Sort counts by class index (0, 1, 2, 3...)\n",
        "    class_counts = [class_counts.get(i, 0) for i in range(len(dataset.classes))]\n",
        "    print(f\"  Class counts: {class_counts}\")\n",
        "    \n",
        "    # Calculate weight per class (1 / num_samples)\n",
        "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "    print(f\"  Class weights: {class_weights}\")\n",
        "    \n",
        "    # Assign a weight to every single sample in the dataset\n",
        "    sample_weights = [class_weights[label] for label in dataset.targets]\n",
        "    \n",
        "    # Create the sampler\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True # Oversampling: draw samples with replacement\n",
        "    )\n",
        "    print(\"WeightedRandomSampler created.\")\n",
        "    return sampler\n",
        "\n",
        "def run_training():\n",
        "    \"\"\"\n",
        "    Main function to run the entire training and evaluation pipeline.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Imbalanced-Aware Training ---\")\n",
        "    \n",
        "    # --- 1. Load Datasets ---\n",
        "    train_transform, val_test_transform = get_xrv_transforms()\n",
        "    \n",
        "    train_dataset = ImageFolder(os.path.join(SPLIT_DATA_DIR, 'train'), transform=train_transform)\n",
        "    val_dataset = ImageFolder(os.path.join(SPLIT_DATA_DIR, 'val'), transform=val_test_transform)\n",
        "    test_dataset = ImageFolder(os.path.join(SPLIT_DATA_DIR, 'test'), transform=val_test_transform)\n",
        "    \n",
        "    # Get class names\n",
        "    class_names = train_dataset.classes\n",
        "    num_classes = len(class_names)\n",
        "    print(f\"\\nTraining on {num_classes} classes: {class_names}\")\n",
        "\n",
        "    # --- 2. Create DataLoaders (with Sampler for train) ---\n",
        "    train_sampler = create_weighted_sampler(train_dataset)\n",
        "    \n",
        "    # Note: shuffle=False because the sampler handles shuffling.\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        sampler=train_sampler, \n",
        "        num_workers=2\n",
        "    )\n",
        "    \n",
        "    # Val and Test loaders should NOT be balanced. We want to test on the\n",
        "    # real, imbalanced distribution.\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        shuffle=False, \n",
        "        num_workers=2\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        shuffle=False, \n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # --- 3. Initialize Model, Loss, Optimizer ---\n",
        "    model = XRVTransferModel(num_classes=num_classes).to(DEVICE)\n",
        "    \n",
        "    # We could also use a weighted loss, but the sampler is often enough.\n",
        "    # If performance is still bad, we can add weighted loss as well.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # We are only training the head, as we froze the backbone\n",
        "    optimizer = optim.Adam(\n",
        "       filter(lambda p: p.requires_grad, model.parameters()), \n",
        "       lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "    # --- 4. Training & Validation Loop ---\n",
        "    best_val_f1 = 0.0\n",
        "    \n",
        "    # Suppress zero-division warnings from sklearn\n",
        "    warnings.filterwarnings('ignore', category=UserWarning, message='F-score is ill-defined')\n",
        "\n",
        "    print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                \n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        # CRITICAL: Use a weighted F1-score, not accuracy\n",
        "        # 'average='weighted'' accounts for class imbalance in the F1 score\n",
        "        val_f1_weighted = f1_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
        "        print(f\"  Val F1 (Weighted): {val_f1_weighted:.4f}\")\n",
        "        \n",
        "        # Save the best model based on F1 score\n",
        "        if val_f1_weighted > best_val_f1:\n",
        "            best_val_f1 = val_f1_weighted\n",
        "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "            print(f\"  New best model saved to {MODEL_SAVE_PATH} (F1: {best_val_f1:.4f})\")\n",
        "            \n",
        "    print(\"\\nTraining complete.\")\n",
        "\n",
        "    # --- 5. Final Evaluation on Test Set ---\n",
        "    print(\"\\n--- FINAL EVALUATION ON TEST SET ---\")\n",
        "    \n",
        "    # Load the *best* model we saved during training\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "        print(f\"Loaded best model from {MODEL_SAVE_PATH}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: No best model was saved. Evaluating last epoch model.\")\n",
        "        \n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Print the final report\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    # This report is the most important output.\n",
        "    # It shows the precision, recall, and f1-score for EACH class.\n",
        "    print(classification_report(all_targets, all_preds, target_names=class_names, zero_division=0))\n",
        "    \n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    # Rows are (True Label), Columns are (Predicted Label)\n",
        "    print(confusion_matrix(all_targets, all_preds))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
