{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec72c5b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchxrayvision as xrv\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8209058",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 1. Model Definition ---\n",
    "class PrototypicalNet(nn.Module):\n",
    "    def __init__(self, out_dim=256): \n",
    "        super(PrototypicalNet, self).__init__()\n",
    "        \n",
    "        # A. Load the default backbone structure\n",
    "        model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "        self.backbone = model.features\n",
    "        \n",
    "        # B. --- THIS IS THE UPGRADE ---\n",
    "        # Load the \"smarter\" weights from our 86% F1-score model\n",
    "        try:\n",
    "            backbone_weights = torch.load('finetuned_backbone_from_imbalanced_model.pth')\n",
    "            self.backbone.load_state_dict(backbone_weights)\n",
    "            print(\"Successfully loaded fine-tuned backbone from imbalanced model.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load fine-tuned backbone. Using default weights. Error: {e}\")\n",
    "        # --------------------------------\n",
    "\n",
    "        # Freeze the (now smarter) backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Add the trainable embedding head\n",
    "        self.embedding_head = nn.Linear(1024, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the *frozen* backbone\n",
    "        with torch.no_grad(): \n",
    "            features = self.backbone(x)\n",
    "            \n",
    "        pooled = self.pooling(features).view(features.size(0), -1)\n",
    "        \n",
    "        # Pass features through the *trainable* head\n",
    "        embedding = self.embedding_head(pooled)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e388d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 2. Data Transforms ---\n",
    "def get_transforms():\n",
    "    XRV_MEAN = [0.5081]\n",
    "    XRV_STD = [0.0893]\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=XRV_MEAN, std=XRV_STD)\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=XRV_MEAN, std=XRV_STD)\n",
    "    ])\n",
    "\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e6a0d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 3. Episodic Sampler ---\n",
    "class EpisodicBatchSampler(Sampler):\n",
    "    def __init__(self, data_targets, n_way, n_shot, n_query, episodes_per_epoch):\n",
    "        super().__init__(data_targets)\n",
    "        self.data_targets = data_targets\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "\n",
    "        self.class_indices = {}\n",
    "        for idx, target in enumerate(self.data_targets):\n",
    "            if target not in self.class_indices:\n",
    "                self.class_indices[target] = []\n",
    "            self.class_indices[target].append(idx)\n",
    "            \n",
    "        self.classes = list(self.class_indices.keys())\n",
    "        \n",
    "        if self.n_way > len(self.classes):\n",
    "            raise ValueError(f\"N_WAY ({self.n_way}) cannot be larger than the number of available classes ({len(self.classes)})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.episodes_per_epoch):\n",
    "            episode_indices = []\n",
    "            \n",
    "            try:\n",
    "                selected_classes = np.random.choice(self.classes, self.n_way, replace=False)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            for cls in selected_classes:\n",
    "                class_idx = self.class_indices[cls]\n",
    "                replace = len(class_idx) < (self.n_shot + self.n_query)\n",
    "                \n",
    "                try:\n",
    "                    selected_idx = np.random.choice(class_idx, self.n_shot + self.n_query, replace=replace)\n",
    "                    episode_indices.extend(selected_idx)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            if len(episode_indices) == self.n_way * (self.n_shot + self.n_query):\n",
    "                yield episode_indices\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6e1e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 4. Prototypical Loss and Accuracy ---\n",
    "def prototypical_loss(embeddings, labels, n_shot, n_query, n_way, device):\n",
    "    embeddings = embeddings.reshape(n_way, n_shot + n_query, -1)\n",
    "    support_embeddings = embeddings[:, :n_shot, :]\n",
    "    query_embeddings = embeddings[:, n_shot:, :,]\n",
    "    \n",
    "    prototypes = support_embeddings.mean(dim=1)\n",
    "    query_embeddings = query_embeddings.reshape(n_way * n_query, -1)\n",
    "    \n",
    "    distances = torch.cdist(query_embeddings, prototypes)\n",
    "    query_labels = torch.arange(n_way, device=device).repeat_interleave(n_query)\n",
    "\n",
    "    loss = F.cross_entropy(-distances, query_labels)\n",
    "    \n",
    "    _, predicted_labels = torch.min(distances, dim=1)\n",
    "    accuracy = (predicted_labels == query_labels).float().mean()\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628990bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 5. Meta-Training Function ---\n",
    "def run_meta_training():\n",
    "    print(\"--- Starting Meta-Training Phase ---\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    META_TRAIN_DIR = 'fsl_data/meta_train'\n",
    "    MODEL_SAVE_PATH = 'fsl_backbone.pth'\n",
    "    N_WAY = 2\n",
    "    N_SHOT = 10\n",
    "    N_QUERY = 10\n",
    "    EPOCHS = 20\n",
    "    EPISODES_PER_EPOCH = 500\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    train_transform, _ = get_transforms()\n",
    "    train_dataset = ImageFolder(META_TRAIN_DIR, transform=train_transform)\n",
    "\n",
    "    print(f\"\\nMeta-Train (Base) dataset loaded.\")\n",
    "    print(f\"Found {len(train_dataset)} images in {len(train_dataset.classes)} classes.\")\n",
    "    print(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "    train_sampler = EpisodicBatchSampler(\n",
    "        train_dataset.targets, N_WAY, N_SHOT, N_QUERY, EPISODES_PER_EPOCH\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_sampler=train_sampler,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    model = PrototypicalNet().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"\\nStarting meta-training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\") as pbar:\n",
    "            for batch in pbar:\n",
    "                images, labels = batch\n",
    "                images = images.to(DEVICE)\n",
    "                \n",
    "                embeddings = model(images)\n",
    "                loss, acc = prototypical_loss(\n",
    "                    embeddings, labels, N_SHOT, N_QUERY, N_WAY, DEVICE\n",
    "                )\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc.item()\n",
    "                \n",
    "                pbar.set_postfix(\n",
    "                    loss=f\"{total_loss / (pbar.n + 1):.4f}\", \n",
    "                    acc=f\"{total_acc / (pbar.n + 1):.4f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_acc = total_acc / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_acc:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"\\nMeta-training complete. Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e900e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 6. Meta-Testing Function ---\n",
    "def run_meta_testing():\n",
    "    print(\"\\n--- Starting Meta-Testing Phase ---\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    META_TEST_DIR = 'fsl_data/meta_test'\n",
    "    MODEL_PATH = 'fsl_backbone.pth'\n",
    "    N_WAY = 2  \n",
    "    N_SHOT = 10 \n",
    "    N_QUERY = 15\n",
    "    TEST_EPISODES = 1000\n",
    "\n",
    "    _, test_transform = get_transforms()\n",
    "    test_dataset = ImageFolder(META_TEST_DIR, transform=test_transform)\n",
    "\n",
    "    print(f\"\\nMeta-Test (Novel) dataset loaded.\")\n",
    "    print(f\"Found {len(test_dataset)} images in {len(test_dataset.classes)} classes.\")\n",
    "    print(f\"Classes: {test_dataset.classes}\")\n",
    "\n",
    "    test_sampler = EpisodicBatchSampler(\n",
    "        data_targets=test_dataset.targets,\n",
    "        n_way=N_WAY,\n",
    "        n_shot=N_SHOT,\n",
    "        n_query=N_QUERY,\n",
    "        episodes_per_epoch=TEST_EPISODES\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_sampler=test_sampler,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    model = PrototypicalNet(out_dim=256).to(DEVICE)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        print(f\"Successfully loaded pre-trained model from {MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch_images, batch_labels) in tqdm(test_loader, desc=\"Running Meta-Test\"):\n",
    "            batch_images = batch_images.to(DEVICE)\n",
    "            \n",
    "            embeddings = model(batch_images)\n",
    "            loss, accuracy = prototypical_loss(\n",
    "                embeddings, batch_labels, N_SHOT, N_QUERY, N_WAY, DEVICE\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_accuracies.append(accuracy.item())\n",
    "\n",
    "    avg_loss = total_loss / TEST_EPISODES\n",
    "    avg_acc = np.mean(all_accuracies)\n",
    "    std_dev = np.std(all_accuracies)\n",
    "    confidence_interval = 1.96 * (std_dev / np.sqrt(len(all_accuracies)))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- Meta-Test Results ---\")\n",
    "    print(f\"Task: {N_WAY}-way, {N_SHOT}-shot (COVID vs TB)\")\n",
    "    print(f\"Episodes Run: {len(all_accuracies)}\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Accuracy: {avg_acc * 100:.2f}%\")\n",
    "    print(f\"95% Confidence Interval: +/- {confidence_interval * 100:.2f}%\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Final Reported Accuracy: {avg_acc * 100:.2f} Â± {confidence_interval * 100:.2f}%\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec452de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_meta_training()\n",
    "    run_meta_testing()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
